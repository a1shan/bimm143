---
title: "Class 08"
author: Alyssa Shan
date: "10/15/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means clustering

example #1 : **kmeans()** function

```{r}
#example plot to see how Rmarkdown works
plot(1:10, typ= "l")
```
back to kmeans...
ctrl+alt+I

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)
```

Use the kmeans() function setting k to 2 and nstart=20
```{r}
k <-  kmeans(x, centers = 2, nstart = 20)
k

```

 How many points are in each cluster?
```{r}
k$size

```
 

What ‘component’ of your result object details
- cluster size?
  `k$size`
- cluster assignment/membership?
  `30, 30`
- cluster center?
  `k$centers`
  
```{r}
k$cluster
table(k$cluster)

k$centers
```


Plot x colored by the kmeans cluster assignment and add cluster centers as blue points
```{r}
palette(c("black", "red"))
plot (x, col=k$cluster)
points(k$centers, col="blue", pch =20, cex=2)
```

 Repeat for k=3, which has the lower tot.withinss?
```{r}
k3 <- kmeans(x, centers = 3, nstart = 20)

k3$tot.withinss
k$tot.withinss
```
k3 has lower tot.withinss

create elbow
```{r}
k2 <- kmeans(x, centers = 2, nstart = 20)
k3 <- kmeans(x, centers = 3, nstart = 20)
k4 <- kmeans(x, centers = 4, nstart = 20)
k5 <- kmeans(x, centers = 5, nstart = 20)

k2$tot.withinss
k3$tot.withinss
k4$tot.withinss
k5$tot.withinss

plot(c(k2$tot.withinss, k3$tot.withinss, k4$tot.withinss, k5$tot.withinss))
```

## hierarchical clustering
trying **hclust ()** in R
example #1:

```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)
```

create membership clusters
```{r}
plot(hc)
abline(h=8, col="red")
cutree(hc, h=8)
#or cutree(hc, k=2)
```

example #2: 
 Step 1. Generate some example data for clustering
```{r}
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")

```
 
Step 2. Plot the data without clustering
```{r}
plot(x)
```
Generate colors for known clusters (just so we can compare to hclust results)
```{r}
col <- as.factor( rep(c("c1","c2","c3"), each=50) )

palette(c("red", "blue", "green"))
plot(x, col=col)
```

Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters 

```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)

cluster2 <- cutree(hc, k=2)
table(cluster2, col)

cluster3 <- cutree(hc, k=3)
table(cluster3, col)

table(col, col)

```

make up data
```{r}
mydata <- matrix(nrow=100, ncol=10) 
rownames(mydata) <- paste("gene", 1:100, sep="") 
colnames(mydata) <- c( paste("wt", 1:5, sep=""),
 paste("ko", 1:5, sep="") ) 
for(i in 1:nrow(mydata)) {
 wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
 ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))

 mydata[i,] <- c(wt.values, ko.values)
}
head(mydata)
```

do PCA
```{r}
pca <- prcomp(t(mydata), scale=TRUE) 
#t() is to transpose data: flip columns to row, etc

```

plot pca in 2d plot
```{r}
# x = pc1 and pc2
plot(pca$x[,1], pca$x[,2])
```

variance in pca
```{r}
pca.var <- pca$sdev^2

#percent variance
pca.var.per <- round(pca.var/sum(pca.var)*100, 1) 
pca.var.per
```

barplot
```{r}
barplot(pca.var.per, main="Scree Plot",
 xlab="Principal Component", ylab="Percent Variation")
```
according to scree plot, big difference btw the two groups along PC1 axis

make PC plot more useful/better to read
```{r}
#create color vector
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

#prettied plot
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
 xlab=paste0("PC1 (", pca.var.per[1], "%)"),
 ylab=paste0("PC2 (", pca.var.per[2], "%)")) 
```

## PCA of UK food data

read data
```{r}
x <- read.csv("UK_foods.csv")
dim(x)

head(x)
```

reset row name so x is not a column
```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)

dim(x)
```

